{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np \n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from tqdm.keras import TqdmCallback\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import deets\n",
    "from choices import get_train_data , param_dict\n",
    "classes = ['AGN' ,'STAR' , 'YSO' ,  'CV' , 'LMXB' , 'HMXB' ,'ULX','PULSAR']\n",
    "flag = {\n",
    "    'conf_flag' : 0 , \n",
    "    'streak_src_flag' : 0 , \n",
    "    'extent_flag' : 0 , \n",
    "    'pileup_flag' : 0 , \n",
    "    }\n",
    "ret_dict =  {\n",
    "    'clf' : False , \n",
    "    'prob_table' : True , \n",
    "    'acc' : True , \n",
    "    'pr_score' : True , \n",
    "    'precision' : True , \n",
    "    'recall' : True , \n",
    "    #'recall' : True ,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offset:  \t0.000|1.010\n",
      "singinficance:  1.000|267.938\n",
      "_____________________________________________________\n",
      "------------------------------\n",
      "Number of Objects : 7703\n",
      "Number of Columns : 47\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "STAR      2790\n",
       "AGN       2395\n",
       "YSO       1149\n",
       "HMXB       748\n",
       "ULX        211\n",
       "CV         166\n",
       "LMXB       143\n",
       "PULSAR     101\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____________________________________________________\n"
     ]
    }
   ],
   "source": [
    "file = f'compiled_data_v3/imputed_data/x_phot_with_color_minmax_modeimp.csv'\n",
    "train_data = get_train_data(flags = flag, classes= classes , offset = 1, file=file , ret_id_cols=['class' , 'significance'])\n",
    "#cols_to_drop = param_dict['hardness']+param_dict['hard_var_col']+param_dict['sparse_col']\n",
    "cols_to_drop = param_dict['hardness']\n",
    "train_data = train_data.drop(columns = cols_to_drop)\n",
    "deets(train_data,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , y_train , sig = train_data.drop(columns=['class' ,'significance']) , train_data[['class']] , train_data['significance']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=10)\n",
    "x_train_pca = pca.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "y_train_oh = encoder.fit_transform(y_train)\n",
    "y_train_oh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history):\n",
    "    fig , ax = plt.subplots(nrows=1 , ncols=2 , figsize= (12,4))\n",
    "\n",
    "    ax[0].plot(history.history['accuracy'] , color='crimson')\n",
    "    ax[0].plot(history.history['val_accuracy'] , color='black')\n",
    "    ax[0].set_title('model accuracy')\n",
    "    ax[0].set_ylabel('accuracy')\n",
    "    ax[0].set_xlabel('epoch')\n",
    "    ax[0].legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    ax[1].plot(history.history['loss'] , color='crimson')\n",
    "    ax[1].plot(history.history['val_loss'] , color='black')\n",
    "    ax[1].set_title('model loss')\n",
    "    ax[1].set_ylabel('loss')\n",
    "    ax[1].set_xlabel('epoch')\n",
    "    ax[1].legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "    #p#lt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(xt ,yt):\n",
    "    inputs = keras.Input(shape=(xt.shape[1],))\n",
    "    #x = layers.BatchNormalization()(inputs)\n",
    "    #x = layers.Activation(\"sigmoid\")(inputs)\n",
    "    x = layers.LayerNormalization()(inputs)\n",
    "    #x = layers.Conv1D(32,3 , input_shape = (xt.shape[1],))(x)\n",
    "    #x = layers.GaussianNoise(0.3)(x)\n",
    "    #x = layers.Flatten()(x)\n",
    "    x = layers.Dense(32, activation='relu')(inputs)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x_mid = layers.Dense(32, activation='relu')(x)\n",
    "    x_mid = layers.LayerNormalization()(x_mid)\n",
    "    x_mid = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.GaussianNoise(0.4)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.GaussianNoise(0.3)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dropout(rate = 0.3)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(inputs)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(inputs)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(inputs)\n",
    "    #x = layers.Dropout(rate = 0.3)(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x_mid = layers.Dropout(rate = 0.8)(x_mid)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    #x = layers.Dropout(rate = 0.3)(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    x = layers.Dense(16, activation='relu')(x)\n",
    "    output = layers.Dense(yt[0].shape[0] , activation='softmax')(x)\n",
    "    model = keras.Model(inputs=inputs , outputs = output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?epoch/s]"
     ]
    }
   ],
   "source": [
    "model = get_model(x_train_pca , y_train_oh)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-3),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=50)\n",
    "mc = ModelCheckpoint('models/fc_net_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)\n",
    "\n",
    "history = model.fit(x_train_pca , y_train_oh ,\n",
    "    verbose=0 , validation_split=0.8,epochs=1000 , callbacks=[es, mc, TqdmCallback(verbose=0)] , \n",
    "    )\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.SVC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-26b784c0fbff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSVC\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.SVC'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn. import svm\n",
    "model = GradientBoostingClassifier()\n",
    "model  = svm()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['class']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train , y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xt,xv , yt,yv = train_test_split(x_train , y_train , stratify = y_train ,  test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GradientBoostingClassifier' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-60c73add21d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GradientBoostingClassifier' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "model.fit(xt,yt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9123945489941596"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(xv,yv)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bc914114b25918618a158fa7997e04b2d9643021d0595e0bf319dbc98b79846"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('tf_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
